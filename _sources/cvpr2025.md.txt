# Seeing Around Corners in Real-Time using Mobile LiDAR

Non-line-of-sight imaging extends perception beyond direct view but has remained impractical on everyday devices. This demo presents a real-time system that uses a commercial mobile LiDAR sensor to track and reconstruct objects hidden around corners. By aggregating multi-bounce LiDAR returns across consecutive frames, our method boosts signal-to-noise ratio and emulates longer exposures, enabling motion-compensated tracking and reconstruction at ~15 Hz. Our system validates the practical implementation of non-line-of-sight imaging on consumer hardware, paving the way for next-generation applications in robotics, autonomous navigation, and augmented reality.

*Aaron Young, Siddharth Somasundaram, Nick Tsao, Nikhil Behari, Akshat Dave, Adithya Pediredla, Ramesh Raskar*

## Project Papers

<div style="display: flex; align-items: center; gap: 1rem;">

<img src="https://camera-culture.github.io/nlos-aided-autonomous-navigation/assets/paper-thumbnail.png" alt="NLOS-Aided Autonomous Navigation" width="100">

**[Enhancing Autonomous Navigation by Imaging Hidden Objects using Single-Photon LiDAR](https://github.com/camera-culture/nlos-aided-autonomous-navigation/blob/main/docs/assets/young2024robospad.pdf)**
[Aaron Young](https://AaronYoung5.github.io/)<sup>\*</sup>, [Nevindu M. Batagoda](https://www.linkedin.com/in/nevindu-b-664a3613b/)<sup>\*</sup>, [Harry Zhang](https://www.linkedin.com/in/haorui-zhang1018), [Akshat Dave](https://akshatdave.github.io/), [Adithya Pediredla](https://sites.google.com/view/adithyapediredla/), [Dan Negrut](https://sbel.wisc.edu/negrut-dan/), [Ramesh Raskar](https://www.media.mit.edu/people/raskar/overview/)

[Paper](https://github.com/camera-culture/nlos-aided-autonomous-navigation/blob/main/docs/assets/young2024robospad.pdf) | [Code](https://github.com/camera-culture/nlos-aided-autonomous-navigation) | [Video](https://youtu.be/0GoUi0wrNMM)

<!-- Robust autonomous navigation in environments with limited visibility remains a critical challenge in robotics. We present a novel approach that leverages Non-Line-of-Sight (NLOS) sensing using single-photon LiDAR to improve visibility and enhance autonomous navigation. Our method enables mobile robots to ``see around corners" by utilizing multi-bounce light information, effectively expanding their perceptual range without additional infrastructure. We propose a three-module pipeline: (1) Sensing, which captures multi-bounce histograms using SPAD-based LiDAR; (2) Perception, which estimates occupancy maps of hidden regions from these histograms using a convolutional neural network; and (3) Control, which allows a robot to follow safe paths based on the estimated occupancy. We evaluate our approach through simulations and real-world experiments on a mobile robot navigating an L-shaped corridor with hidden obstacles. Our work represents the first experimental demonstration of NLOS imaging for autonomous navigation, paving the way for safer and more efficient robotic systems operating in complex environments. We also contribute a novel dynamics-integrated transient rendering framework for simulating NLOS scenarios, facilitating future research in this domain. -->

</div>

## Citation

If you would like to cite this demo, please use the following BibTeX entry:

```bibtex
@inproceedings{young2024enhancing,
    author = {Young, Aaron and Batagoda, Nevindu M. and Zhang, Harry and Dave, Akshat and Pediredla, Adithya and Negrut, Dan and Raskar, Ramesh},
    title = {Enhancing Autonomous Navigation by Imaging Hidden Objects using Single-Photon LiDAR},
    booktitle = {ArXiv},
    year = {2024}
}
```

```bibtex
@software{young2025seeing,
    author = {Aaron Young and Siddharth Somasundaram and Nick Tsao and Nikhil Behari and Akshat Dave and Adithya Pediredla and Ramesh Raskar},
    title = {{Seeing Around Corners in Real-Time using Mobile LiDAR}},
    year = {2025},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/camera-culture/cc-demos}},
}
```
